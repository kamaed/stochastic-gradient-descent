# Gradient descent

Implementation of algos from the [An overview of gradient descent optimization algorithms (arXiv:1609.04747)](https://arxiv.org/abs/1609.04747)

* Momentum
* Nesterov accelerated gradient (NAG)
* Adagrad
* Adadelta
* RMSprop
* Adam
* AdaMax
* Nadam (NAG + Adam)
